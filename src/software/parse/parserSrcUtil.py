#!/usr/bin/python3
# -*- coding: utf-8 -*-
# *****************************************************************************/
# * Authors: Randal Eike, Joseph Tarango
# *****************************************************************************/
from __future__ import absolute_import, division, print_function, unicode_literals  # , nested_scopes, generators, generator_stop, with_statement, annotations
# General Python module imports
import sys
from collections import deque

class tokenList(object):
    """
    Token list handler
    """
    DELIMITER_TOKEN  = 0 # Chars such as the following ; () [] ::
    OPERATOR_TOKEN   = 1 # Numerical Operator + - / *
    IDENTIFIER_TOKEN = 2 # Other bucket
    ML_COMMENT_TOKEN = 3 # Multi-lines such as /* */
    SL_COMMENT_TOKEN = 4 # Single line such as //
    STRING_TOKEN     = 5 # Following a comment token or follow a quote ("") ('')
    WHITESPACE_TOKEN = 6 # Space
    EOL_TOKEN        = 7 # New line \n

    def __init__(self):
        self.tokenList = []
        self.currentTokenIndex = 0
        pass

    def clear(self):
        del self.tokenList[0:]
        self.currentTokenIndex = 0

    def resetTokenPull(self):
        self.currentTokenIndex = 0

    def addToken(self, tokenType, tokenValue):
        self.tokenList.append((tokenType, tokenValue))

    def getNextToken(self):
        """
        Read the next token from the list of tokens generated by the input stream
        """
        if (self.currentTokenIndex < len(self.tokenList)):
            tokenType, token = self.tokenList[self.currentTokenIndex]
            self.currentTokenIndex += 1
        else:
            token = ""
            tokenType = tokenList.EOL_TOKEN
        return tokenType, token

    def getPreviewToken(self):
        """
        Read the next token from the list of tokens generated by the input stream without moving index.
        """
        if (self.currentTokenIndex < len(self.tokenList)):
            tokenType, token = self.tokenList[self.currentTokenIndex]
        else:
            token = ""
            tokenType = tokenList.EOL_TOKEN
        return tokenType, token

    def putToken(self):
        """
        Adjust the token list pointer back one token
        """
        if (self.currentTokenIndex > 0): self.currentTokenIndex -= 1

    def isEndofList(self):
        """
        Check if we have reached the end of the token list
        """
        if (self.currentTokenIndex < len(self.tokenList)): return False
        else: return True


class fileTokenizer(tokenList):
    """
    Tokenize structure definition file on a character-by-character level
    """
    def resetTokenizer(self):
        self.nextChar = None
        self.currentToken = None
        self.currentTokenType = tokenList.IDENTIFIER_TOKEN

        del self.tokenList[0:]
        self.resetTokenPull()
        self.mlStartToken = ""

        self.startofLine = True

    def __init__(self, operatorList = None, delimiterList = None, stringOpList = None, slCommentStartList = None, mlCommentStartList = None, mlCommentEnd = None, continuationCharacter = '\\', stripWhite = True):
        self.inputStream = None

        self.operatorList = operatorList
        self.linebreakList = ['\n', '\l', '\r', '\f']
        self.whiteList = [' ', '\t']
        self.escapeList = ['\0', '\a']
        self.stripWhite = stripWhite
        self.continuationCharacter = continuationCharacter

        self.delimiterList = delimiterList
        self.stringOperatorList = stringOpList

        if (mlCommentStartList is not None):
            self.mlCommentStartList = mlCommentStartList
            self.mlCommentEndToken = mlCommentEnd
            self.mlCommentEnabled = True
        else:
            self.mlCommentStartList = []
            self.mlCommentEndToken = None
            self.mlCommentEnabled = False

        if (slCommentStartList is not None):
            self.slCommentStartList = slCommentStartList
            self.slCommentEnabled = True
        else:
            self.slCommentStartList = []
            self.slCommentEnabled = False

        self.nextChar = None
        self.currentToken = None
        self.currentTokenType = tokenList.IDENTIFIER_TOKEN

        self.tokenList = []
        self.resetTokenizer()

    #================================================================
    #================================================================
    # Character operations
    #================================================================
    #================================================================
    def __isDelimiter(self):
        if(self.nextChar is not None):
            if (self.nextChar in self.delimiterList or self.nextChar in self.stringOperatorList): return True
            else: return False
        else: return False

    def __isWhiteSpace(self):
        if(self.nextChar is not None):
            if (self.nextChar in self.whiteList): return True
            else: return False
        else: return False

    def __isEscapeChar(self):
        if(self.nextChar is not None):
            if (self.nextChar in self.escapeList): return True
            else: return False
        else: return False

    def __isLineBreak(self):
        if(self.nextChar is not None):
            if (self.nextChar in self.linebreakList): return True
            else: return False
        else: return False

    def __isOperator(self):
        if(self.nextChar is not None):
            # String Operator should not be grouped with isOperator
            #if ((self.nextChar in self.operatorList) or (self.nextChar in self.stringOperatorList)): return True
            if ((self.nextChar in self.operatorList)): return True
            else: return False
        else: return False


    def __isIdentifier(self):
        if(self.nextChar is not None):
            if ((self.nextChar in self.operatorList) or \
                (self.nextChar in self.stringOperatorList) or \
                (self.nextChar in self.whiteList) or \
                (self.nextChar in self.linebreakList) or \
                (self.nextChar in self.delimiterList)): return False
            else: return True
        else: return False

    def __getNextChar(self):
        self.nextChar = self.inputStream.read(1)
        if (self.nextChar == ''): self.nextChar = None

    def __putChar(self):
        self.inputStream.seek(self.inputStream.tell()-1, 0)

    #================================================================
    #================================================================
    # Stream operations
    #================================================================
    #================================================================
    def __stripWitespace(self):
        token = ""
        # strip white space
        while ((self.nextChar is not None) and (self.__isWhiteSpace())):
            token += self.nextChar
            self.__getNextChar()

        # if the first whitespace on the line and not an empty line and strip is false
        if ((self.startofLine) and (False == self.stripWhite) and (False == self.__isLineBreak())):
            # First white space on the line, add it to the token and clear the start of line flag
            self.currentToken = token
            self.startofLine = False

    def __stripContinuation(self):
        # strip to the end of the line, then strip leading while space
        while ((self.nextChar is not None) and (False == self.__isLineBreak())): self.__getNextChar()
        while ((self.nextChar is not None) and (self.__isWhiteSpace())): self.__getNextChar()

    def __pullToken(self):
        if (self.nextChar is not None):
            if (self.nextChar == self.continuationCharacter): self.__stripContinuation()

            # Get the next charater for the token
            if(self.currentToken is None): self.currentToken = self.nextChar
            else: self.currentToken += self.nextChar
            self.__getNextChar()

        if (self.__isLineBreak()): self.startofLine = True

    #================================================================
    #================================================================
    # Token operations
    #================================================================
    #================================================================
    def __pullOperator(self):
        """
        Pull an operator token from the input stream
        """
        self.currentTokenType = fileTokenizer.OPERATOR_TOKEN
        while ((self.nextChar is not None) and (True == self.__isOperator())):
            self.__pullToken()

    def __pullIdentifier(self):
        """
        Pull an identifier token from the input stream
        """
        self.currentTokenType = fileTokenizer.IDENTIFIER_TOKEN
        while ((self.nextChar is not None) and (True == self.__isIdentifier())):
            self.__pullToken()

    def __isMultiCommentStartDelimiter(self, token):
        comment = False
        self.mlStartToken = ""
        if(self.mlCommentEnabled):
            for commentStart in self.mlCommentStartList:
                cmpLength = len(commentStart)
                if (len(token) >= cmpLength):
                    if (token[:cmpLength] == commentStart):
                        comment = True
                        self.mlStartToken = token[:cmpLength]
                        break
        return comment

    def __isSingleCommentStartDelimiter(self, token):
        comment = False
        if(self.slCommentEnabled):
            for commentStart in self.slCommentStartList:
                cmpLength = len(commentStart)
                if (len(token) >= cmpLength):
                    if (token[:cmpLength] == commentStart):
                        comment = True
                        break
        return comment

    def __pullMultiLineComment(self):
        """
        Pull a multiline comment token from the input stream
        """
        self.currentTokenType = tokenList.ML_COMMENT_TOKEN
        foundEnd = False
        if (self.mlCommentEndToken is None):
            mlCommentEndToken = self.mlStartToken
            mlCommentEndTokenLen = len(mlCommentEndToken)
            mlCommentMinLen = mlCommentEndTokenLen * 2
        else:
            mlCommentEndToken = self.mlCommentEnd
            mlCommentEndTokenLen = len(self.mlCommentEnd)
            mlCommentMinLen = mlCommentEndTokenLen + len(self.currentToken)

        tokencmpIndex = 0 - mlCommentEndTokenLen

        while ((self.nextChar is not None) and (False == foundEnd)):
            self.__pullToken()
            if (len(self.currentToken) >= mlCommentMinLen):
                if (self.currentToken[tokencmpIndex:] == mlCommentEndToken):
                    foundEnd = True

    def __pullSingleLineComment(self):
        """
        Pull a single line comment token from the input stream
        """
        self.currentTokenType = tokenList.SL_COMMENT_TOKEN
        while ((self.nextChar is not None) and (False == self.__isLineBreak())):
            self.__pullToken()

    def __pullString(self):
        """
        Pull a string token from the input stream
        """
        self.currentTokenType = tokenList.STRING_TOKEN
        self.currentToken = self.nextChar
        endChar = self.nextChar
        # Ensure not looking at same char twice
        self.__getNextChar()

        # While not end of string. Ignore the continuation char and let the parser deal with it
        while ((self.nextChar is not None) and (self.nextChar != endChar) and (False == self.__isLineBreak())):
            self.currentToken += self.nextChar
            self.__getNextChar()

        # put the closing token onto the string
        if(False == self.__isLineBreak()):
            self.currentToken += self.nextChar
            self.__getNextChar()

    #================================================================
    #================================================================
    # Token generation
    #================================================================
    #================================================================
    def __tokenize(self):
        """
        Generate a token from the input stream
        """
        self.currentToken = None

        # strip white space and put the first non-whitespace charater into the token
        self.__stripWitespace()

        if(self.nextChar is not None):
            # get the token and ending delimiter
            if (self.__isLineBreak()):
                self.startofLine = True
                self.__getNextChar()

            elif (self.__isDelimiter()):
                self.currentToken = self.nextChar
                self.currentTokenType = tokenList.DELIMITER_TOKEN

                # Check for string delimiter
                if (self.currentToken in self.stringOperatorList): self.__pullString()
                else: self.__getNextChar()

            elif (self.__isOperator()):
                # get the full operator
                self.__pullOperator()
                if (self.__isMultiCommentStartDelimiter(self.currentToken)):
                    self.__pullMultiLineComment()
                elif (self.__isSingleCommentStartDelimiter(self.currentToken)):
                    self.__pullSingleLineComment()
            else:
                # get identifier
                self.__pullIdentifier()

    def parseStream(self, inputStream):
        """
        Generate the token list from the input file
        """
        self.resetTokenizer()
        self.inputStream = inputStream
        self.__getNextChar()

        while (self.nextChar is not None):
            self.__tokenize()
            if (self.currentToken is not None):
                self.addToken(self.currentTokenType, self.currentToken)

        self.tokenCount = len(self.tokenList)


class numericParser(object):
    """
    Parse and validate numeric token
    """
    DigitList = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
    OctalDigitList = ['0', '1', '2', '3', '4', '5', '6', '7']
    BinDigitList = ['0', '1']
    HexDigitList = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'A', 'B', 'C', 'D', 'E', 'F']

    def __init__(self):
        self.tokenNumericValue = 0
        return super(numericParser, self).__init__()

    #================================================================
    #================================================================
    # Token operations
    #================================================================
    #================================================================
    def __validateAndConvertNumber(self, number, digitList, radix):
        isNumber = True
        self.tokenNumericValue = 0
        for c in number:
            if(c not in digitList):
                isNumber = False
                break

        # convert the data
        if(isNumber): self.tokenNumericValue = int(number, radix)
        #if radix == 16: print("%s\n"%self.tokenNumericValue) #@remove
        return isNumber

    def isNumber(self, token):
        if (token is None):
            isNumericValue = False
        elif (token[0] in numericParser.DigitList):
            isNumericValue = True
            number = token.upper()

            # Determine radix
            if ((len(number) > 2) and (number[0] == '0')):
                if (number[1].lower() in ['x', 'o', 'b', 'd']):
                    base = number[1].lower()
                    number = number[2:]
            elif (number[-1:].lower() in ['o', 'b', 'd']):
                # check last character
                base = number[-1:].lower()
                number = number[0:-1]
            else:
                # assume decimal number for now
                base = 'd'
                isNumericValue = True

            # validate and convert
            if(isNumericValue):
                if(base == 'x'):
                    #print ("is number! %s\n"% number) #@remove
                    isNumericValue = self.__validateAndConvertNumber(number, numericParser.HexDigitList, 16)
                elif (base == 'o'):
                    isNumericValue = self.__validateAndConvertNumber(number, numericParser.OctalDigitList, 8)
                elif (base == 'b'):
                    isNumericValue = self.__validateAndConvertNumber(number, numericParser.BinDigitList, 2)
                elif (base == 'd'):
                    isNumericValue = self.__validateAndConvertNumber(number, numericParser.DigitList, 10)
                else:
                    isNumericValue = False
        else:
            # First character must be digit
            isNumericValue = False

        return isNumericValue

    def getValue(self, token):
        if (self.isNumber(token)): return self.tokenNumericValue
        else: return 0


class parserHelper(numericParser):
    """
    File parser helper
    """
    DEFAULT_TOKEN      = 0 #
    IDENTIFIER_TOKEN   = 1 #
    KEYWORD_TOKEN      = 2 #
    OPERATOR_TOKEN     = 3 #
    PREPROCESSOR_TOKEN = 4 #
    COMMENT_TOKEN      = 5 #
    NUMERIC_TOKEN      = 6 #
    STRING_TOKEN       = 7 #
    DELIMITER_TOKEN    = 8 #
    END_OF_LIST        = 9 #

    def __init__(self, tokenizer, keywordList = None, preprocessorKey = None):
        self.errorCount = 0
        self.errorAbortCount = 1
        self.deferredNodeQueue = deque()

        self.previousToken = ""
        self.currentToken = ""
        self.endOfListFound = False
        self.tokenType = parserHelper.DEFAULT_TOKEN

        self.tokenizer = tokenizer
        self.keyWordList = keywordList
        self.preprocessorKey = preprocessorKey

        self.printWarning = False

        return super(parserHelper, self).__init__()

    def resetParser(self):
        self.errorCount = 0
        self.errorAbortCount = 1

        self.previousToken = ""
        self.currentToken = ""
        self.endOfListFound = False
        self.tokenType = parserHelper.DEFAULT_TOKEN

    def parseError(self, formatStr, args = None):
        self.errorCount += 1
        if (self.tokenizer.isEndofList()):
            sys.stderr.write("Unexpected end of token list reached\n")

        errorStr = format(formatStr % args)
        sys.stderr.write(errorStr)
        sys.stderr.write("\n")

    def parseWarning(self, formatStr, args = None):
        warningStr = format(formatStr % args)
        if (self.printWarning):
            sys.stdout.write(warningStr)
            sys.stdout.write("\n")

    def parseInformation(self, formatStr, args = None):
        informationStr = format(formatStr % args)
        sys.stdout.write(informationStr)
        sys.stdout.write("\n")

    def continueParse(self):
        if ((self.errorCount < self.errorAbortCount) and (False == self.tokenizer.isEndofList())): return True
        else: return False

    #================================================================
    #================================================================
    # Token operations
    #================================================================
    #================================================================

    def __isKeyWord(self):
        # Compare identifier against keyword list
        if (self.keyWordList is not None):
            if (self.currentToken in self.keyWordList): return True
            else: return False
        else: return False

    def __isPreprocessorKey(self):
        # Compare identifier against operator list
        if (self.preprocessorKey is not None):
            if (self.currentToken[0] == self.preprocessorKey): return True
            else: return False
        else: return False

    def isValidIdentifier(self):
        return True

    def putToken(self):
        self.tokenizer.putToken()

    def getNextToken(self):
        # get the next token from the tokenizer
        self.previousToken = self.currentToken
        tokenType, self.currentToken = self.tokenizer.getNextToken()

        # Determine the token type
        if (tokenType == fileTokenizer.DELIMITER_TOKEN):
            self.tokenType = parserHelper.DELIMITER_TOKEN

        elif (tokenType == fileTokenizer.IDENTIFIER_TOKEN):
            if (self.__isKeyWord()): self.tokenType = parserHelper.KEYWORD_TOKEN
            elif (self.currentToken[0] in numericParser.DigitList): self.tokenType = parserHelper.NUMERIC_TOKEN
            elif (self.isValidIdentifier()): self.tokenType = parserHelper.IDENTIFIER_TOKEN
            else: self.tokenType = parserHelper.DEFAULT_TOKEN

        elif ((tokenType == fileTokenizer.ML_COMMENT_TOKEN) or (tokenType == fileTokenizer.SL_COMMENT_TOKEN)):
            self.tokenType = parserHelper.COMMENT_TOKEN

        elif (tokenType == fileTokenizer.OPERATOR_TOKEN):
            if (self.__isPreprocessorKey()): self.tokenType = parserHelper.PREPROCESSOR_TOKEN
            else: self.tokenType = parserHelper.OPERATOR_TOKEN

        elif (tokenType == fileTokenizer.STRING_TOKEN):
            self.tokenType = parserHelper.STRING_TOKEN

        elif (tokenType == fileTokenizer.EOL_TOKEN):
            self.tokenType = parserHelper.END_OF_LIST

        else: self.tokenType = parserHelper.DEFAULT_TOKEN

    def getIgnoreToken(self):
        # get the next token from the tokenizer and dont update variables used for function pointers!
        tokenType, currentToken = self.tokenizer.getNextToken()
        return tokenType, currentToken

    def getPreviewToken(self):
        # get the next token from the tokenizer without change
        tokenType, currentToken = self.tokenizer.getPreviewToken() # Dont move the index!
        return tokenType, currentToken

    def createNode(self):
        newNode = tokenList()
        self.deferredNodeQueue.append(newNode)
        return newNode

    def getNextNode(self):
        try:
            node = self.deferredNodeQueue.popleft()
        except:
            node = None
        return node


